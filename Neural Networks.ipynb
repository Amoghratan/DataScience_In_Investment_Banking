{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "### Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-39\n"
     ]
    }
   ],
   "source": [
    "input_data=np.array([3, 5])\n",
    "weights={'node_0': np.array([2, 4]), 'node_1': np.array([ 4, -5]), 'output': np.array([2, 7])}\n",
    "\n",
    "\n",
    "node_0_value=(input_data * weights['node_0']).sum()\n",
    "node_1_value=(input_data * weights['node_1']).sum()\n",
    "hidden_layer_output=np.array([node_0_value,node_1_value])\n",
    "\n",
    "output=(hidden_layer_output*weights['output']).sum()\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](NN1.png)\n",
    "\n",
    "### The Rectified Linear Activation Function\n",
    "\n",
    "relu(3)=3<br>\n",
    "relu(-3)=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "def relu(input):\n",
    "    return max(0,input)\n",
    "\n",
    "node_0_value=(input_data * weights['node_0']).sum()\n",
    "node_0_value=relu(node_0_value)\n",
    "\n",
    "node_1_value=(input_data * weights['node_1']).sum()\n",
    "node_1_value=relu(node_1_value)\n",
    "\n",
    "hidden_layer_output=np.array([node_0_value,node_1_value])\n",
    "\n",
    "output=(hidden_layer_output*weights['output']).sum()\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Giving the model above multiple inputs and checking for results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52, 63, 0, 148]\n"
     ]
    }
   ],
   "source": [
    "# Define predict_with_network()\n",
    "input_data=[np.array([3, 5]), np.array([ 1, -1]), np.array([0, 0]), np.array([8, 4])]\n",
    "weights={'node_0': np.array([2, 4]), 'node_1': np.array([ 4, -5]), 'output': np.array([2, 7])}\n",
    "\n",
    "\n",
    "def predict_with_network(input_data_row, weights):\n",
    "    # Calculate node 0 value\n",
    "    node_0_input = (input_data_row * weights['node_0']).sum()\n",
    "    node_0_output = relu(node_0_input)\n",
    "\n",
    "    # Calculate node 1 value\n",
    "    node_1_input = (input_data_row*weights['node_1']).sum()\n",
    "    node_1_output = relu(node_1_input)\n",
    "\n",
    "    # Put node values into array: hidden_layer_outputs\n",
    "    hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "    \n",
    "    # Calculate model output\n",
    "    input_to_final_layer = (hidden_layer_outputs*weights['output']).sum()\n",
    "    model_output = relu(input_to_final_layer)\n",
    "    \n",
    "    # Return model output\n",
    "    return(model_output)\n",
    "\n",
    "\n",
    "# Create empty list to store prediction results\n",
    "results = []\n",
    "for input_data_row in input_data:\n",
    "    # Append prediction to results\n",
    "    results.append(predict_with_network(input_data_row, weights))\n",
    "\n",
    "# Print results\n",
    "print(results)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mult-layer input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182\n"
     ]
    }
   ],
   "source": [
    "input_data = np.array([3, 5])\n",
    "\n",
    "weights = {'node_0_0': np.array([2, 4]),\n",
    " 'node_0_1': np.array([ 4, -5]),\n",
    " 'node_1_0': np.array([-1,  2]),\n",
    " 'node_1_1': np.array([1, 2]),\n",
    " 'output': np.array([2, 7])}\n",
    "\n",
    "def predict_with_network(input_data):\n",
    "    # Calculate node 0 in the first hidden layer\n",
    "    node_0_0_input = (input_data * weights['node_0_0']).sum()\n",
    "    node_0_0_output = relu(node_0_0_input)\n",
    "\n",
    "    # Calculate node 1 in the first hidden layer\n",
    "    node_0_1_input = (input_data * weights['node_0_1']).sum()\n",
    "    node_0_1_output = relu(node_0_1_input)\n",
    "\n",
    "    # Put node values into array: hidden_0_outputs\n",
    "    hidden_0_outputs = np.array([node_0_0_output, node_0_1_output])\n",
    "    \n",
    "    # Calculate node 0 in the second hidden layer\n",
    "    node_1_0_input = (hidden_0_outputs*weights['node_1_0']).sum()\n",
    "    node_1_0_output = relu(node_1_0_input)\n",
    "\n",
    "    # Calculate node 1 in the second hidden layer\n",
    "    node_1_1_input = (hidden_0_outputs*weights['node_1_1']).sum()\n",
    "    node_1_1_output = relu(node_1_1_input)\n",
    "\n",
    "    # Put node values into array: hidden_1_outputs\n",
    "    hidden_1_outputs = np.array([node_1_0_output, node_1_1_output])\n",
    "\n",
    "    # Calculate model output: model_output\n",
    "    model_output = (hidden_1_outputs*weights['output']).sum()\n",
    "    \n",
    "    # Return model_output\n",
    "    return(model_output)\n",
    "\n",
    "output = predict_with_network(input_data)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"Multi-layer Network\"](NN2.png)\n",
    "\n",
    "1) Tha last layers in the NN capture the most complex relationships and interactions<br>\n",
    "2) The model training process sets weights to optimize predictive accuracy\n",
    "\n",
    "### Gradient Descent and making better predictions by Changing weights\n",
    "\n",
    "1) Slope of mean square function= 2 * (error(taregt-prediction)) * (value feeding into the node)<br>\n",
    "2) New weight would be 2(original weight) - 0.01(learning rate)* (Slope of mean square func)=new weight<br>\n",
    "\n",
    "1) Slope of mean square function = 2 * -4 * 3 = -24<br>\n",
    "2) New Weight = 2 - (0.01)*-24<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "5.04\n"
     ]
    }
   ],
   "source": [
    "input_data=np.array([1, 2, 3])\n",
    "weights=np.array([0, 2, 1])\n",
    "target=0\n",
    "\n",
    "# Set the learning rate: learning_rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Calculate the predictions: preds\n",
    "preds = (weights * input_data).sum()\n",
    "\n",
    "# Calculate the error: error\n",
    "error = preds - target\n",
    "\n",
    "# Calculate the slope: slope\n",
    "slope = 2 * input_data * error\n",
    "\n",
    "# Update the weights: weights_updated\n",
    "weights_updated = weights - slope*learning_rate\n",
    "\n",
    "# Get updated predictions: preds_updated\n",
    "preds_updated = (weights_updated*input_data).sum()\n",
    "\n",
    "# Calculate updated error: error_updated\n",
    "error_updated = preds_updated-target\n",
    "\n",
    "# Print the original error\n",
    "print(error)\n",
    "\n",
    "# Print the updated error\n",
    "print(error_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras\n",
    "\n",
    "1) Specify Architercture - Layers, nodes/layer and type of activation func<br>\n",
    "2) Compile - Loss func, details about optimization<br>\n",
    "3) Fit - Backpropagation & optimization<br>\n",
    "4) Predict <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.1 ],\n",
       "       [ 4.95],\n",
       "       [ 6.67],\n",
       "       [ 4.  ],\n",
       "       [ 7.5 ],\n",
       "       [13.07],\n",
       "       [ 4.45],\n",
       "       [19.47],\n",
       "       [13.28],\n",
       "       [ 8.75],\n",
       "       [11.35],\n",
       "       [11.5 ],\n",
       "       [ 6.5 ],\n",
       "       [ 6.25],\n",
       "       [19.98],\n",
       "       [ 7.3 ],\n",
       "       [ 8.  ],\n",
       "       [22.2 ],\n",
       "       [ 3.65],\n",
       "       [20.55],\n",
       "       [ 5.71],\n",
       "       [ 7.  ],\n",
       "       [ 3.75],\n",
       "       [ 4.5 ],\n",
       "       [ 9.56],\n",
       "       [ 5.75],\n",
       "       [ 9.36],\n",
       "       [ 6.5 ],\n",
       "       [ 3.35],\n",
       "       [ 4.75],\n",
       "       [ 8.9 ],\n",
       "       [ 4.  ],\n",
       "       [ 4.7 ],\n",
       "       [ 5.  ],\n",
       "       [ 9.25],\n",
       "       [10.67],\n",
       "       [ 7.61],\n",
       "       [10.  ],\n",
       "       [ 7.5 ],\n",
       "       [12.2 ],\n",
       "       [ 3.35],\n",
       "       [11.  ],\n",
       "       [12.  ],\n",
       "       [ 4.85],\n",
       "       [ 4.3 ],\n",
       "       [ 6.  ],\n",
       "       [15.  ],\n",
       "       [ 4.85],\n",
       "       [ 9.  ],\n",
       "       [ 6.36],\n",
       "       [ 9.15],\n",
       "       [11.  ],\n",
       "       [ 4.5 ],\n",
       "       [ 4.8 ],\n",
       "       [ 4.  ],\n",
       "       [ 5.5 ],\n",
       "       [ 8.4 ],\n",
       "       [ 6.75],\n",
       "       [10.  ],\n",
       "       [ 5.  ],\n",
       "       [ 6.5 ],\n",
       "       [10.75],\n",
       "       [ 7.  ],\n",
       "       [11.43],\n",
       "       [ 4.  ],\n",
       "       [ 9.  ],\n",
       "       [13.  ],\n",
       "       [12.22],\n",
       "       [ 6.28],\n",
       "       [ 6.75],\n",
       "       [ 3.35],\n",
       "       [16.  ],\n",
       "       [ 5.25],\n",
       "       [ 3.5 ],\n",
       "       [ 4.22],\n",
       "       [ 3.  ],\n",
       "       [ 4.  ],\n",
       "       [10.  ],\n",
       "       [ 5.  ],\n",
       "       [16.  ],\n",
       "       [13.98],\n",
       "       [13.26],\n",
       "       [ 6.1 ],\n",
       "       [ 3.75],\n",
       "       [ 9.  ],\n",
       "       [ 9.45],\n",
       "       [ 5.5 ],\n",
       "       [ 8.93],\n",
       "       [ 6.25],\n",
       "       [ 9.75],\n",
       "       [ 6.73],\n",
       "       [ 7.78],\n",
       "       [ 2.85],\n",
       "       [ 3.35],\n",
       "       [19.98],\n",
       "       [ 8.5 ],\n",
       "       [ 9.75],\n",
       "       [15.  ],\n",
       "       [ 8.  ],\n",
       "       [11.25],\n",
       "       [14.  ],\n",
       "       [10.  ],\n",
       "       [ 6.5 ],\n",
       "       [ 9.83],\n",
       "       [18.5 ],\n",
       "       [12.5 ],\n",
       "       [26.  ],\n",
       "       [14.  ],\n",
       "       [10.5 ],\n",
       "       [11.  ],\n",
       "       [12.47],\n",
       "       [12.5 ],\n",
       "       [15.  ],\n",
       "       [ 6.  ],\n",
       "       [ 9.5 ],\n",
       "       [ 5.  ],\n",
       "       [ 3.75],\n",
       "       [12.57],\n",
       "       [ 6.88],\n",
       "       [ 5.5 ],\n",
       "       [ 7.  ],\n",
       "       [ 4.5 ],\n",
       "       [ 6.5 ],\n",
       "       [12.  ],\n",
       "       [ 5.  ],\n",
       "       [ 6.5 ],\n",
       "       [ 6.8 ],\n",
       "       [ 8.75],\n",
       "       [ 3.75],\n",
       "       [ 4.5 ],\n",
       "       [ 6.  ],\n",
       "       [ 5.5 ],\n",
       "       [13.  ],\n",
       "       [ 5.65],\n",
       "       [ 4.8 ],\n",
       "       [ 7.  ],\n",
       "       [ 5.25],\n",
       "       [ 3.35],\n",
       "       [ 8.5 ],\n",
       "       [ 6.  ],\n",
       "       [ 6.75],\n",
       "       [ 8.89],\n",
       "       [14.21],\n",
       "       [10.78],\n",
       "       [ 8.9 ],\n",
       "       [ 7.5 ],\n",
       "       [ 4.5 ],\n",
       "       [11.25],\n",
       "       [13.45],\n",
       "       [ 6.  ],\n",
       "       [ 4.62],\n",
       "       [10.58],\n",
       "       [ 5.  ],\n",
       "       [ 8.2 ],\n",
       "       [ 6.25],\n",
       "       [ 8.5 ],\n",
       "       [24.98],\n",
       "       [16.65],\n",
       "       [ 6.25],\n",
       "       [ 4.55],\n",
       "       [11.25],\n",
       "       [21.25],\n",
       "       [12.65],\n",
       "       [ 7.5 ],\n",
       "       [10.25],\n",
       "       [ 3.35],\n",
       "       [13.45],\n",
       "       [ 4.84],\n",
       "       [26.29],\n",
       "       [ 6.58],\n",
       "       [44.5 ],\n",
       "       [15.  ],\n",
       "       [11.25],\n",
       "       [ 7.  ],\n",
       "       [10.  ],\n",
       "       [14.53],\n",
       "       [20.  ],\n",
       "       [22.5 ],\n",
       "       [ 3.64],\n",
       "       [10.62],\n",
       "       [24.98],\n",
       "       [ 6.  ],\n",
       "       [19.  ],\n",
       "       [13.2 ],\n",
       "       [22.5 ],\n",
       "       [15.  ],\n",
       "       [ 6.88],\n",
       "       [11.84],\n",
       "       [16.14],\n",
       "       [13.95],\n",
       "       [13.16],\n",
       "       [ 5.3 ],\n",
       "       [ 4.5 ],\n",
       "       [10.  ],\n",
       "       [10.  ],\n",
       "       [10.  ],\n",
       "       [ 9.37],\n",
       "       [ 5.8 ],\n",
       "       [17.86],\n",
       "       [ 1.  ],\n",
       "       [ 8.8 ],\n",
       "       [ 9.  ],\n",
       "       [18.16],\n",
       "       [ 7.81],\n",
       "       [10.62],\n",
       "       [ 4.5 ],\n",
       "       [17.25],\n",
       "       [10.5 ],\n",
       "       [ 9.22],\n",
       "       [15.  ],\n",
       "       [22.5 ],\n",
       "       [ 4.55],\n",
       "       [ 9.  ],\n",
       "       [13.33],\n",
       "       [15.  ],\n",
       "       [ 7.5 ],\n",
       "       [ 4.25],\n",
       "       [12.5 ],\n",
       "       [ 5.13],\n",
       "       [ 3.35],\n",
       "       [11.11],\n",
       "       [ 3.84],\n",
       "       [ 6.4 ],\n",
       "       [ 5.56],\n",
       "       [10.  ],\n",
       "       [ 5.65],\n",
       "       [11.5 ],\n",
       "       [ 3.5 ],\n",
       "       [ 3.35],\n",
       "       [ 4.75],\n",
       "       [19.98],\n",
       "       [ 3.5 ],\n",
       "       [ 4.  ],\n",
       "       [ 7.  ],\n",
       "       [ 6.25],\n",
       "       [ 4.5 ],\n",
       "       [14.29],\n",
       "       [ 5.  ],\n",
       "       [13.75],\n",
       "       [13.71],\n",
       "       [ 7.5 ],\n",
       "       [ 3.8 ],\n",
       "       [ 5.  ],\n",
       "       [ 9.42],\n",
       "       [ 5.5 ],\n",
       "       [ 3.75],\n",
       "       [ 3.5 ],\n",
       "       [ 5.8 ],\n",
       "       [12.  ],\n",
       "       [ 5.  ],\n",
       "       [ 8.75],\n",
       "       [10.  ],\n",
       "       [ 8.5 ],\n",
       "       [ 8.63],\n",
       "       [ 9.  ],\n",
       "       [ 5.5 ],\n",
       "       [11.11],\n",
       "       [10.  ],\n",
       "       [ 5.2 ],\n",
       "       [ 8.  ],\n",
       "       [ 3.56],\n",
       "       [ 5.2 ],\n",
       "       [11.67],\n",
       "       [11.32],\n",
       "       [ 7.5 ],\n",
       "       [ 5.5 ],\n",
       "       [ 5.  ],\n",
       "       [ 7.75],\n",
       "       [ 5.25],\n",
       "       [ 9.  ],\n",
       "       [ 9.65],\n",
       "       [ 5.21],\n",
       "       [ 7.  ],\n",
       "       [12.16],\n",
       "       [ 5.25],\n",
       "       [10.32],\n",
       "       [ 3.35],\n",
       "       [ 7.7 ],\n",
       "       [ 9.17],\n",
       "       [ 8.43],\n",
       "       [ 4.  ],\n",
       "       [ 4.13],\n",
       "       [ 3.  ],\n",
       "       [ 4.25],\n",
       "       [ 7.53],\n",
       "       [10.53],\n",
       "       [ 5.  ],\n",
       "       [15.03],\n",
       "       [11.25],\n",
       "       [ 6.25],\n",
       "       [ 3.5 ],\n",
       "       [ 6.85],\n",
       "       [12.5 ],\n",
       "       [12.  ],\n",
       "       [ 6.  ],\n",
       "       [ 9.5 ],\n",
       "       [ 4.1 ],\n",
       "       [10.43],\n",
       "       [ 5.  ],\n",
       "       [ 7.69],\n",
       "       [ 5.5 ],\n",
       "       [ 6.4 ],\n",
       "       [12.5 ],\n",
       "       [ 6.25],\n",
       "       [ 8.  ],\n",
       "       [ 9.6 ],\n",
       "       [ 9.1 ],\n",
       "       [ 7.5 ],\n",
       "       [ 5.  ],\n",
       "       [ 7.  ],\n",
       "       [ 3.55],\n",
       "       [ 8.5 ],\n",
       "       [ 4.5 ],\n",
       "       [ 7.88],\n",
       "       [ 5.25],\n",
       "       [ 5.  ],\n",
       "       [ 9.33],\n",
       "       [10.5 ],\n",
       "       [ 7.5 ],\n",
       "       [ 9.5 ],\n",
       "       [ 9.6 ],\n",
       "       [ 5.87],\n",
       "       [11.02],\n",
       "       [ 5.  ],\n",
       "       [ 5.62],\n",
       "       [12.5 ],\n",
       "       [10.81],\n",
       "       [ 5.4 ],\n",
       "       [ 7.  ],\n",
       "       [ 4.59],\n",
       "       [ 6.  ],\n",
       "       [11.71],\n",
       "       [ 5.62],\n",
       "       [ 5.5 ],\n",
       "       [ 4.85],\n",
       "       [ 6.75],\n",
       "       [ 4.25],\n",
       "       [ 5.75],\n",
       "       [ 3.5 ],\n",
       "       [ 3.35],\n",
       "       [10.62],\n",
       "       [ 8.  ],\n",
       "       [ 4.75],\n",
       "       [ 8.5 ],\n",
       "       [ 8.85],\n",
       "       [ 8.  ],\n",
       "       [ 6.  ],\n",
       "       [ 7.14],\n",
       "       [ 3.4 ],\n",
       "       [ 6.  ],\n",
       "       [ 3.75],\n",
       "       [ 8.89],\n",
       "       [ 4.35],\n",
       "       [13.1 ],\n",
       "       [ 4.35],\n",
       "       [ 3.5 ],\n",
       "       [ 3.8 ],\n",
       "       [ 5.26],\n",
       "       [ 3.35],\n",
       "       [16.26],\n",
       "       [ 4.25],\n",
       "       [ 4.5 ],\n",
       "       [ 8.  ],\n",
       "       [ 4.  ],\n",
       "       [ 7.96],\n",
       "       [ 4.  ],\n",
       "       [ 4.15],\n",
       "       [ 5.95],\n",
       "       [ 3.6 ],\n",
       "       [ 8.75],\n",
       "       [ 3.4 ],\n",
       "       [ 4.28],\n",
       "       [ 5.35],\n",
       "       [ 5.  ],\n",
       "       [ 7.65],\n",
       "       [ 6.94],\n",
       "       [ 7.5 ],\n",
       "       [ 3.6 ],\n",
       "       [ 1.75],\n",
       "       [ 3.45],\n",
       "       [ 9.63],\n",
       "       [ 8.49],\n",
       "       [ 8.99],\n",
       "       [ 3.65],\n",
       "       [ 3.5 ],\n",
       "       [ 3.43],\n",
       "       [ 5.5 ],\n",
       "       [ 6.93],\n",
       "       [ 3.51],\n",
       "       [ 3.75],\n",
       "       [ 4.17],\n",
       "       [ 9.57],\n",
       "       [14.67],\n",
       "       [12.5 ],\n",
       "       [ 5.5 ],\n",
       "       [ 5.15],\n",
       "       [ 8.  ],\n",
       "       [ 5.83],\n",
       "       [ 3.35],\n",
       "       [ 7.  ],\n",
       "       [10.  ],\n",
       "       [ 8.  ],\n",
       "       [ 6.88],\n",
       "       [ 5.55],\n",
       "       [ 7.5 ],\n",
       "       [ 8.93],\n",
       "       [ 9.  ],\n",
       "       [ 3.5 ],\n",
       "       [ 5.77],\n",
       "       [25.  ],\n",
       "       [ 6.85],\n",
       "       [ 6.5 ],\n",
       "       [ 3.75],\n",
       "       [ 3.5 ],\n",
       "       [ 4.5 ],\n",
       "       [ 2.01],\n",
       "       [ 4.17],\n",
       "       [13.  ],\n",
       "       [ 3.98],\n",
       "       [ 7.5 ],\n",
       "       [13.12],\n",
       "       [ 4.  ],\n",
       "       [ 3.95],\n",
       "       [13.  ],\n",
       "       [ 9.  ],\n",
       "       [ 4.55],\n",
       "       [ 9.5 ],\n",
       "       [ 4.5 ],\n",
       "       [ 8.75],\n",
       "       [10.  ],\n",
       "       [18.  ],\n",
       "       [24.98],\n",
       "       [12.05],\n",
       "       [22.  ],\n",
       "       [ 8.75],\n",
       "       [22.2 ],\n",
       "       [17.25],\n",
       "       [ 6.  ],\n",
       "       [ 8.06],\n",
       "       [ 9.24],\n",
       "       [12.  ],\n",
       "       [10.61],\n",
       "       [ 5.71],\n",
       "       [10.  ],\n",
       "       [17.5 ],\n",
       "       [15.  ],\n",
       "       [ 7.78],\n",
       "       [ 7.8 ],\n",
       "       [10.  ],\n",
       "       [24.98],\n",
       "       [10.28],\n",
       "       [15.  ],\n",
       "       [12.  ],\n",
       "       [10.58],\n",
       "       [ 5.85],\n",
       "       [11.22],\n",
       "       [ 8.56],\n",
       "       [13.89],\n",
       "       [ 5.71],\n",
       "       [15.79],\n",
       "       [ 7.5 ],\n",
       "       [11.25],\n",
       "       [ 6.15],\n",
       "       [13.45],\n",
       "       [ 6.25],\n",
       "       [ 6.5 ],\n",
       "       [12.  ],\n",
       "       [ 8.5 ],\n",
       "       [ 8.  ],\n",
       "       [ 5.75],\n",
       "       [15.73],\n",
       "       [ 9.86],\n",
       "       [13.51],\n",
       "       [ 5.4 ],\n",
       "       [ 6.25],\n",
       "       [ 5.5 ],\n",
       "       [ 5.  ],\n",
       "       [ 6.25],\n",
       "       [ 5.75],\n",
       "       [20.5 ],\n",
       "       [ 5.  ],\n",
       "       [ 7.  ],\n",
       "       [18.  ],\n",
       "       [12.  ],\n",
       "       [20.4 ],\n",
       "       [22.2 ],\n",
       "       [16.42],\n",
       "       [ 8.63],\n",
       "       [19.38],\n",
       "       [14.  ],\n",
       "       [10.  ],\n",
       "       [15.95],\n",
       "       [20.  ],\n",
       "       [10.  ],\n",
       "       [24.98],\n",
       "       [11.25],\n",
       "       [22.83],\n",
       "       [10.2 ],\n",
       "       [10.  ],\n",
       "       [14.  ],\n",
       "       [12.5 ],\n",
       "       [ 5.79],\n",
       "       [24.98],\n",
       "       [ 4.35],\n",
       "       [11.25],\n",
       "       [ 6.67],\n",
       "       [ 8.  ],\n",
       "       [18.16],\n",
       "       [12.  ],\n",
       "       [ 8.89],\n",
       "       [ 9.5 ],\n",
       "       [13.65],\n",
       "       [12.  ],\n",
       "       [15.  ],\n",
       "       [12.67],\n",
       "       [ 7.38],\n",
       "       [15.56],\n",
       "       [ 7.45],\n",
       "       [ 6.25],\n",
       "       [ 6.25],\n",
       "       [ 9.37],\n",
       "       [22.5 ],\n",
       "       [ 7.5 ],\n",
       "       [ 7.  ],\n",
       "       [ 5.75],\n",
       "       [ 7.67],\n",
       "       [12.5 ],\n",
       "       [16.  ],\n",
       "       [11.79],\n",
       "       [11.36],\n",
       "       [ 6.1 ],\n",
       "       [23.25],\n",
       "       [19.88],\n",
       "       [15.38]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('hourly_wages.csv')\n",
    "features=data[['union','education_yrs','experience_yrs','age','female','marr','south','manufacturing','construction']]\n",
    "target=data[['wage_per_hour']]\n",
    "\n",
    "features=np.array(features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "n_cols=features.shape[1]\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Dense(50,activation='relu',input_shape=(n_cols,)))\n",
    "model.add(Dense(36,activation='relu'))\n",
    "model.add(Dense(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling a model \n",
    "![\"Compiling a model\"](NN3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "534/534 [==============================] - 0s 717us/step - loss: 82.7924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb243b5ef0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Fit the model\n",
    "model.fit(features,target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "534/534 [==============================] - 0s 422us/step - loss: 83.9752\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb24971908>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the model\n",
    "n_cols = features.shape[1]\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape = (n_cols,)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Fit the model\n",
    "model.fit(features, target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Models\n",
    "\n",
    "1) ‘categorical_crossentropy’ loss function <br>\n",
    "2) Similar to log loss: Lower is be er<br>\n",
    "3) Add metrics = [‘accuracy’] to compile step for easy-to- understand diagnostics<br>\n",
    "4) Output layer has separate node for each possible outcome, and uses ‘so max’ activation<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 11 columns):\n",
      "survived                     891 non-null int64\n",
      "pclass                       891 non-null int64\n",
      "age                          891 non-null float64\n",
      "sibsp                        891 non-null int64\n",
      "parch                        891 non-null int64\n",
      "fare                         891 non-null float64\n",
      "male                         891 non-null int64\n",
      "age_was_missing              891 non-null bool\n",
      "embarked_from_cherbourg      891 non-null int64\n",
      "embarked_from_queenstown     891 non-null int64\n",
      "embarked_from_southampton    891 non-null int64\n",
      "dtypes: bool(1), float64(2), int64(8)\n",
      "memory usage: 70.6 KB\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv('titanic_all_numeric.csv')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "891/891 [==============================] - 0s 338us/step - loss: 3.6281 - acc: 0.6049\n",
      "[3.3164131e+01 1.0000000e+02 2.3727282e+01 1.0000000e+02 2.1683910e+00\n",
      " 1.0884809e+01 1.0000000e+02 1.0000000e+02 7.4217644e+01 1.0000000e+02\n",
      " 9.9999832e+01 9.6786613e+01 5.9433670e+01 9.9999916e+01 8.9218079e+01\n",
      " 3.1659615e+00 1.0000000e+02 7.8669861e+01 9.9206299e+01 7.6878896e+00\n",
      " 9.9994888e+01 5.0750687e+01 8.5514297e+01 1.0000000e+02 9.9999985e+01\n",
      " 9.9999962e+01 6.2737660e+00 1.0000000e+02 9.0421801e+00 8.5665312e+00\n",
      " 9.9993095e+01 1.0000000e+02 8.2930450e+00 2.1588195e-03 1.0000000e+02\n",
      " 1.0000000e+02 6.2918916e+00 5.2560520e+01 9.9991821e+01 9.9235016e+01\n",
      " 2.0211568e+00 9.9981888e+01 9.8643322e+00 1.0000000e+02 6.4311813e+01\n",
      " 9.4941368e+00 9.5525047e+01 8.2930450e+00 9.9975784e+01 9.9990723e+01\n",
      " 1.0000000e+02 4.7983395e+01 1.0000000e+02 9.9999725e+01 1.0000000e+02\n",
      " 1.0000000e+02 8.8504692e+01 8.0227156e+00 1.0000000e+02 1.0000000e+02\n",
      " 3.4908016e+01 1.0000000e+02 1.0000000e+02 1.0000000e+02 9.9999962e+01\n",
      " 9.6439316e+01 4.5348057e+01 6.7715004e+01 8.4180054e+01 3.2721474e+01\n",
      " 2.2385794e+01 1.0000000e+02 1.0000000e+02 9.7374222e+01 1.0000000e+02\n",
      " 2.1277346e+01 8.5665312e+00 9.4941368e+00 1.0000000e+02 7.4062637e+01\n",
      " 6.2699001e+01 2.5537916e+01 8.5044680e+00 1.0000000e+02 9.5774826e+01\n",
      " 9.4643059e+01 1.0000000e+02 9.4941368e+00 1.0000000e+02 3.2393913e+01\n",
      " 1.0595050e+01 5.5932587e+01 1.0000000e+02 9.9983810e+01 2.0046514e-03\n",
      " 9.4941368e+00 9.9642525e+01 1.0000000e+02 9.9944458e+01 9.9996658e+01\n",
      " 1.4831045e+01 8.5665312e+00 1.0000000e+02 5.6912866e+00 1.2934517e+00\n",
      " 1.2277769e+01 5.0695789e+01 7.8980522e+00 8.4885818e-01 9.9997238e+01\n",
      " 1.0000000e+02 9.9958435e+01 4.5588951e+01 8.7692955e+01 9.9895844e+01\n",
      " 5.0272541e+01 9.6354117e-05 9.9943787e+01 1.0000000e+02 1.0000000e+02\n",
      " 1.0000000e+02 9.4941368e+00 9.9999977e+01 6.6096970e+01 1.0000000e+02\n",
      " 9.9609276e+01 6.7757764e+00 1.9759478e+01 9.9991676e+01 6.6459544e-02\n",
      " 3.8862493e+00 4.1315224e+01 1.0393100e+01 9.9999725e+01 9.2682335e+01\n",
      " 9.9095222e+01 1.0000000e+02 1.0000000e+02 9.1223198e+01 1.0000000e+02\n",
      " 9.7023712e+01 4.5554459e+01 9.9444527e+01 3.9211048e+01 9.6645805e+01\n",
      " 1.0000000e+02 1.4675786e+01 1.0000000e+02 9.9991539e+01 9.9278231e+00\n",
      " 6.2550271e-01 1.0000000e+02 7.7189077e-03 3.8559467e+01 5.7582302e+00\n",
      " 1.0000000e+02 7.8498161e+01 8.2246981e+00 1.4113966e+01 1.0000000e+02\n",
      " 4.1391945e+01 6.4277367e+01 1.8300924e+01 8.4144821e+01 1.0000000e+02\n",
      " 9.9999985e+01 1.0000000e+02 9.9976570e+01 9.9999741e+01 1.0000000e+02\n",
      " 9.9935600e+01 1.0000000e+02 9.9985229e+01 5.0272541e+01 9.9895416e+01\n",
      " 7.1834167e+01 9.9999367e+01 9.9930557e+01 7.5908676e+01 1.2377817e-02\n",
      " 1.0000000e+02 9.5007248e+01 1.0000000e+02 1.0000000e+02 1.0000000e+02\n",
      " 1.0000000e+02 9.6313248e+01 9.9879395e+01 5.6107269e+01 1.4749540e+00\n",
      " 6.9153931e+01 9.8461143e+01 6.8970451e+01 1.0000000e+02 9.9972145e+01\n",
      " 1.0000000e+02 6.7757764e+00 4.3683383e-01 8.2930450e+00 9.5306023e+01\n",
      " 3.1201565e+01 1.0000000e+02 9.2955643e-01 7.5175278e-02 7.1925049e+01\n",
      " 9.9976608e+01 9.4309593e+01 9.9908951e+01 7.8698128e+01 9.9999786e+01\n",
      " 1.8715576e+01 9.9678246e+01 3.1795204e+01 7.5908676e+01 7.1809368e+00\n",
      " 1.0000000e+02 1.9044809e+01 9.9966583e+01 1.0000000e+02 3.3525776e+01\n",
      " 8.1751053e+01 8.7931023e+01 2.5350900e-02 8.5665312e+00 1.0000000e+02\n",
      " 6.8478081e+01 9.1521660e+01 4.1481834e+01 9.8974487e+01 9.9999451e+01\n",
      " 1.0000000e+02 8.8319521e+00 1.3737431e-01 1.0000000e+02 7.2947754e+01\n",
      " 8.2966719e+00 9.9882545e+01 1.0000000e+02 9.1521660e+01 4.4475410e+01\n",
      " 9.4643661e+01 9.6313248e+01 4.0009045e+01 2.9842516e+01 5.4091840e+00\n",
      " 1.0000000e+02 2.6929390e+01 9.8536743e+01 1.0000000e+02 9.8256256e+01\n",
      " 5.5146441e+00 4.9818363e+01 8.8785965e+01 9.7190811e+01 9.7708008e+01\n",
      " 9.7385788e+01 1.0000000e+02 1.0000000e+02 1.0000000e+02 9.9508873e+01\n",
      " 6.7757764e+00 1.0000000e+02 1.0000000e+02 3.3206434e-03 8.2930450e+00\n",
      " 8.6197557e+00 1.0000000e+02 2.3971367e+01 1.0000000e+02 1.0000000e+02\n",
      " 1.0000000e+02 2.1555974e-01 9.5746765e+01 9.9999832e+01 8.2930450e+00\n",
      " 1.0000000e+02 1.3520561e-01 5.8868311e-02 1.0000000e+02 9.9557327e+01\n",
      " 4.0444639e-04 1.1953163e+01 9.2709213e+01 6.5956009e+01 9.9999748e+01\n",
      " 6.6223874e+00 2.0594595e+01 4.2802574e+01 9.9278231e+00 4.1937405e+01\n",
      " 1.0000000e+02 1.0000000e+02 3.8578949e+01 5.1725346e+01 2.9969820e+01\n",
      " 9.9999962e+01 2.6072493e+01 1.0000000e+02 1.0000000e+02 1.0000000e+02\n",
      " 8.2930450e+00 9.9991486e+01 1.0172178e+00 7.0528564e+01 9.4941368e+00\n",
      " 1.0000000e+02 1.0000000e+02 1.0000000e+02 9.9997223e+01 1.0000000e+02\n",
      " 1.0000000e+02 1.0000000e+02 9.9999939e+01 1.2277769e+01 9.9929680e+01\n",
      " 2.2801096e+01 9.9999962e+01 7.9599088e-01 1.0000000e+02 1.0000000e+02\n",
      " 3.1795204e+01 1.5617430e+01 6.7613853e+01 1.0000000e+02 1.0000000e+02\n",
      " 1.0000000e+02 6.8577757e-04 4.2303570e+01 9.9875977e+01 1.0000000e+02\n",
      " 9.9992493e+01 9.9965858e+01 1.0000000e+02 9.9996201e+01 1.0000000e+02\n",
      " 8.5665312e+00 1.0000000e+02 1.0000000e+02 1.3539353e-01 9.9999977e+01\n",
      " 1.0000000e+02 1.0000000e+02 8.4638206e+01 9.2682335e+01 3.7079777e+01\n",
      " 9.5306023e+01 1.9341446e+01 9.7981171e+01 9.9999687e+01 4.8870659e-01\n",
      " 5.9984249e+01 1.0000000e+02 8.1333099e+01 9.9777031e+01 6.2737660e+00\n",
      " 3.1201565e+01 1.0000000e+02 2.9543167e+01 9.0421801e+00 9.0421801e+00\n",
      " 9.9994232e+01 9.9999962e+01 1.8853960e+01 1.0538419e+00 9.5525047e+01\n",
      " 4.7493753e+00 1.0000000e+02 7.7097631e+00 8.2930450e+00 1.0000000e+02\n",
      " 1.0000000e+02 4.5593483e+01 6.5956009e+01 1.0000000e+02 1.0000000e+02\n",
      " 1.0000000e+02 3.6708843e+01 1.0000000e+02 8.4407263e+00 6.1296551e+01\n",
      " 1.0000000e+02 9.9999825e+01 4.4675136e+00 1.0000000e+02 8.5665312e+00\n",
      " 1.0000000e+02 1.0000000e+02 4.2303570e+01 6.6801205e+00 9.8865982e+01\n",
      " 1.0000000e+02 4.7906578e+01 1.3941875e+01 1.0000000e+02 9.9710396e+01\n",
      " 4.1018562e+01 6.8075285e+00 9.9781403e+01 7.8097992e+01 8.4137161e+01\n",
      " 6.5708447e-01 2.1508327e+01 8.4346748e+01 9.8027100e+01 7.4063629e+01\n",
      " 9.9723000e+01 2.1198658e-02 9.9999977e+01 4.7526291e+01 9.9999451e+01\n",
      " 8.5665312e+00 3.6433437e+00 1.0000000e+02 5.8868311e-02 1.6331978e-01\n",
      " 1.1544723e+01 1.0000000e+02 9.9298744e+01 7.5908676e+01 1.0000000e+02\n",
      " 9.8643322e+00 4.3127720e+01 9.4401283e+00 9.5855904e+01 9.9999199e+01\n",
      " 5.5146441e+00 9.9999825e+01 1.0000000e+02 6.7757764e+00 4.8752108e+00\n",
      " 9.9999908e+01 9.7981171e+01 9.9944023e+01 6.3233631e+01 1.0000000e+02\n",
      " 1.0000000e+02 1.0000000e+02 9.9966408e+01 1.0000000e+02 2.7609554e+01\n",
      " 9.9901436e+01 8.0916389e+01 2.3971367e+01 8.7269218e+01 9.8950911e+00\n",
      " 1.0000000e+02 9.9999870e+01 9.9998413e+01 9.9999977e+01 9.9952003e+01\n",
      " 9.9998878e+01 9.9839439e+01 9.9999954e+01 1.0000000e+02 9.4941368e+00\n",
      " 1.1002955e+01 7.7396263e+01 1.0000000e+02 2.3423052e-01 6.7757764e+00\n",
      " 9.9729340e+01 2.8474724e+00 1.0000000e+02 2.0198469e+00 9.4941368e+00\n",
      " 4.5849910e-01 5.8868311e-02 9.7692215e+01 6.6609645e+00 9.9999985e+01\n",
      " 5.5146441e+00 1.4794197e+00 9.9999786e+01 9.8210930e+01 7.9438736e+01\n",
      " 1.0000000e+02 9.9723000e+01 5.6984210e+00 3.6245205e+01 9.9995171e+01\n",
      " 1.0000000e+02 5.8868311e-02 3.3520021e-02 3.4053479e-03 1.0000000e+02\n",
      " 9.9999451e+01 1.0000000e+02 9.9636383e+01 8.2246981e+00 9.9998711e+01\n",
      " 9.9839439e+01 3.8135826e+01 9.9892189e+01 1.0000000e+02 5.2560520e+01\n",
      " 9.3024948e+01 1.0000000e+02 9.4716034e+01 1.0000000e+02 2.8454575e+01\n",
      " 8.4144821e+01 4.8851913e+01 7.6441298e+00 4.6405239e+00 1.0000000e+02\n",
      " 1.0000000e+02 9.9998703e+01 9.9999855e+01 9.9993004e+01 1.0000000e+02\n",
      " 7.5879083e+00 9.4941368e+00 9.9994217e+01 1.0000000e+02 2.4197449e+01\n",
      " 9.9999619e+01 1.7027546e+01 9.9997116e+01 9.9991844e+01 4.3770704e+00\n",
      " 1.0000000e+02 4.2802574e+01 6.2737660e+00 1.0000000e+02 6.2918916e+00\n",
      " 3.2920671e-01 2.3423052e-01 1.0000000e+02 6.5708447e-01 9.0075897e+01\n",
      " 1.0000000e+02 6.2918916e+00 7.1360870e+01 9.9993370e+01 1.4869852e+01\n",
      " 1.0000000e+02 9.9879395e+01 1.0000000e+02 9.2122528e+01 1.0000000e+02\n",
      " 1.0000000e+02 1.0000000e+02 1.0000000e+02 9.9998787e+01 1.0000000e+02\n",
      " 7.5159882e+01 1.0000000e+02 8.9018188e+01 9.9739922e+01 1.0000000e+02\n",
      " 1.0000000e+02 9.9999878e+01 7.1517482e+00 3.4838104e+01 4.6009258e+01\n",
      " 8.8785965e+01 1.0000000e+02 1.0000000e+02 1.0000000e+02 9.5454773e+01\n",
      " 6.7757764e+00 4.8721632e-01 8.8824493e+01 9.4941368e+00 1.1544723e+01\n",
      " 9.9999702e+01 6.3374722e+01 9.9973480e+01 6.2918916e+00 4.2512217e+00\n",
      " 6.6006300e-03 1.0000000e+02 9.9994812e+01 8.2930450e+00 8.1751053e+01\n",
      " 9.9605652e+01 5.6180744e+01 1.0000000e+02 9.4658432e+01 4.4675136e+00\n",
      " 1.0000000e+02 1.0000000e+02 9.8136482e+01 1.0000000e+02 1.6605417e+01\n",
      " 1.0000000e+02 1.0559123e+01 1.0000000e+02 4.5588951e+01 9.4941368e+00\n",
      " 1.1127249e+00 1.0000000e+02 4.5825198e-02 9.5345945e+00 9.9984756e+01\n",
      " 9.9955627e+01 1.0000000e+02 4.2179876e-04 6.2737660e+00 1.0000000e+02\n",
      " 9.9999985e+01 8.5665312e+00 1.0000000e+02 1.7895986e-01 9.9998047e+01\n",
      " 8.1310158e+01 7.4109674e+00 1.0000000e+02 1.0000000e+02 1.0000000e+02\n",
      " 9.9999931e+01 4.7988710e+00 9.6313248e+01 6.7757764e+00 2.1683910e+00\n",
      " 1.0000000e+02 7.7952202e+01 9.9204185e+01 1.0000000e+02 6.0662563e+01\n",
      " 9.6568581e+01 1.0000000e+02 9.9884460e+01 4.8975487e+01 9.9806992e+01\n",
      " 9.9851044e+01 8.9314468e-02 1.0000000e+02 1.9661905e+01 6.6988764e+00\n",
      " 3.9365974e+01 1.4516963e-02 9.9999985e+01 4.9858574e-02 1.0000000e+02\n",
      " 8.7269218e+01 4.4675136e+00 9.9999489e+01 1.0000000e+02 9.7540459e+01\n",
      " 5.5932587e+01 1.0000000e+02 1.0000000e+02 1.0000000e+02 9.9999985e+01\n",
      " 1.0000000e+02 6.3374722e+01 9.9996582e+01 6.7787924e+00 3.5336533e+01\n",
      " 8.5665312e+00 9.9999962e+01 5.9469765e+01 8.7452974e+00 5.1376808e+01\n",
      " 1.0000000e+02 8.5665312e+00 9.3413673e+01 9.5600975e+01 1.0000000e+02\n",
      " 1.0000000e+02 3.4857357e-01 9.9590622e+01 1.1043240e+00 5.8728264e+01\n",
      " 1.0000000e+02 9.2682335e+01 7.8980522e+00 2.3651147e-01 1.0000000e+02\n",
      " 1.0000000e+02 1.0000000e+02 7.0604798e-04 7.0438477e+01 5.8868311e-02\n",
      " 6.7682259e+01 2.9412508e+01 9.2042549e+01 1.0000000e+02 1.0000000e+02\n",
      " 1.0724507e+01 1.0000000e+02 7.7609390e+01 1.0000000e+02 9.9999214e+01\n",
      " 1.0000000e+02 1.0000000e+02 9.0118675e+01 6.8014778e+01 1.0000000e+02\n",
      " 1.0000000e+02 9.9998024e+01 1.0000000e+02 1.8779514e+01 9.3263481e+01\n",
      " 9.6322608e-01 1.7895986e-01 8.2004366e+00 1.0000000e+02 2.3329242e-01\n",
      " 1.0000000e+02 9.9996521e+01 9.9863663e+01 1.9970564e+01 2.0171354e+01\n",
      " 9.9966904e+01 7.8811936e+00 9.9935837e+01 1.0000000e+02 9.6439316e+01\n",
      " 1.0000000e+02 9.9999855e+01 1.0000000e+02 2.5305885e+01 6.6968846e-01\n",
      " 5.9102066e+01 1.0000000e+02 5.9199230e+01 9.5159950e+01 3.0709920e+00\n",
      " 1.0000000e+02 6.3043064e+01 5.0750687e+01 1.1651875e+00 1.0000000e+02\n",
      " 6.9652542e+01 9.9937721e+01 8.2236414e+00 9.9999939e+01 3.0452374e+01\n",
      " 1.0000000e+02 9.9999870e+01 5.8868311e-02 9.5600975e+01 9.5600975e+01\n",
      " 9.7949669e+01 9.9999664e+01 1.0000000e+02 8.5665312e+00 8.5665312e+00\n",
      " 1.0000000e+02 1.0000000e+02 1.0000000e+02 9.9431099e+01 5.8237634e+00\n",
      " 1.0000000e+02 9.9999687e+01 7.9675751e+01 1.0000000e+02 4.4850802e+00\n",
      " 1.0000000e+02 9.9990372e+01 1.0085372e+01 3.6139305e+01 1.0000000e+02\n",
      " 9.9999275e+01 1.1510020e+01 9.6645805e+01 2.8474724e+00 1.0000000e+02\n",
      " 9.2122528e+01 2.1001539e-01 4.8394047e+01 1.0000000e+02 7.8550354e+01\n",
      " 1.0000000e+02 1.0000000e+02 6.2952862e+00 9.9996803e+01 6.0539908e+00\n",
      " 5.8101360e+01 5.0769676e-02 3.3196148e-02 6.2737660e+00 9.0730049e+01\n",
      " 6.7280197e+01 6.7757764e+00 9.9993385e+01 6.7181416e+00 1.0000000e+02\n",
      " 8.9084290e+01 1.0000000e+02 1.0000000e+02 9.9996391e+01 1.4829829e+01\n",
      " 1.6778090e+01 6.7984535e+01 1.0000000e+02 1.0000000e+02 1.0000000e+02\n",
      " 6.7757764e+00 1.0000000e+02 1.0000000e+02 1.0000000e+02 2.4450298e+01\n",
      " 2.0309977e+01 9.9550148e+01 1.1825974e+01 5.4249592e+00 9.9997284e+01\n",
      " 5.0750687e+01 9.9999557e+01 1.0000000e+02 9.9900764e+01 8.6132298e+00\n",
      " 5.2493086e+00 4.3063508e-03 7.2265694e+01 2.0309977e+01 1.0000000e+02\n",
      " 1.9565992e+01 9.9884453e+01 1.1090192e+01 1.0000000e+02 7.2298408e+00\n",
      " 4.9858574e-02 4.1837414e+01 1.0000000e+02 8.4126242e-02 1.0000000e+02\n",
      " 1.0000000e+02 2.4507019e+01 5.5846409e-03 8.7638077e+01 1.0000000e+02\n",
      " 3.8867633e+00 1.0000000e+02 1.0000000e+02 6.7757764e+00 1.0000000e+02\n",
      " 9.9948799e+01 9.9999977e+01 6.2918916e+00 3.5438698e+01 7.5472343e+01\n",
      " 1.0000000e+02 6.3445896e+01 9.4941368e+00 1.0000000e+02 1.0000000e+02\n",
      " 5.7207279e+01 9.6041557e+01 1.0000000e+02 9.0495151e-01 8.4144821e+01\n",
      " 2.1684231e-01 1.0000000e+02 2.2600732e+00 1.0000000e+02 1.0000000e+02\n",
      " 1.0000000e+02 5.3341944e-05 9.9998383e+01 1.0000000e+02 9.9904007e+01\n",
      " 8.9694641e+01 1.0000000e+02 9.9393723e+01 9.9987900e+01 6.2918916e+00\n",
      " 2.8476971e+01 9.3248550e+01 9.9656204e+01 1.0000000e+02 9.4315178e+01\n",
      " 1.2059602e+01 9.5483429e+01 1.0000000e+02 2.3288782e+01 9.9978050e+01\n",
      " 1.9661905e+01 1.0000000e+02 3.6066452e-01 1.5530461e-01 9.9999069e+01\n",
      " 8.2465286e+01 8.4527519e+01 6.3374722e+01 8.5665312e+00 1.0000000e+02\n",
      " 9.9999962e+01 3.3457565e+00 8.6398605e+01 4.6862232e+01 1.4829829e+01\n",
      " 9.9999107e+01 8.7931023e+01 1.0000000e+02 9.9996780e+01 1.0000000e+02\n",
      " 3.4292414e+00]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "predictors=data.drop(['survived'],axis=1).values\n",
    "target=to_categorical(data.survived)\n",
    "\n",
    "n_cols=predictors.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first layer\n",
    "model.add(Dense(32,activation='relu',input_shape=(n_cols,)))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(predictors,target)\n",
    "\n",
    "\n",
    "predictions=model.predict(predictors)\n",
    "\n",
    "predicted_prob_true = predictions[:,1]\n",
    "\n",
    "# print predicted_prob_true\n",
    "print(predicted_prob_true*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Model Optimization\n",
    "\n",
    "Best Optimizer : Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the SGD optimizer\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Create list of learning rates: lr_to_test\n",
    "lr_to_test = [0.000001,0.01,1]\n",
    "\n",
    "# Loop over learning rates\n",
    "for lr in lr_to_test:\n",
    "    print('\\n\\nTesting model with learning rate: %f\\n'%lr )\n",
    "    \n",
    "    # Build new model to test, unaffected by previous models\n",
    "    model = get_new_model()\n",
    "    \n",
    "    # Create SGD optimizer with specified learning rate: my_optimizer\n",
    "    my_optimizer = SGD(lr=lr)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=my_optimizer,loss='categorical_crossentropy')\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(predictors,target)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding validation split instead of Cross Fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the number of columns in predictors: n_cols\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "n_cols = predictors.shape[1]\n",
    "input_shape = (n_cols,)\n",
    "\n",
    "# Specify the model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape = input_shape))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Define early_stopping_monitor\n",
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "hist = model.fit(predictors,target,epochs=30,validation_split=0.3,callbacks=[early_stopping_monitor])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define early_stopping_monitor\n",
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "\n",
    "# Create the new model: model_2\n",
    "model_2 = Sequential()\n",
    "\n",
    "# Add the first and second layers\n",
    "model_2.add(Dense(100, activation='relu', input_shape=input_shape))\n",
    "model_2.add(Dense(100, activation='relu', input_shape=input_shape))\n",
    "\n",
    "# Add the output layer\n",
    "model_2.add(Dense(2,activation='softmax'))\n",
    "\n",
    "# Compile model_2\n",
    "model_2.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# Fit model_1\n",
    "model_1_training = model_1.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n",
    "\n",
    "# Fit model_2\n",
    "model_2_training = model_2.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n",
    "\n",
    "# Create the plot\n",
    "plt.plot(model_1_training.history['val_loss'], 'r', model_2_training.history['val_loss'], 'b')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation score')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model: model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first hidden layer\n",
    "model.add(Dense(50,activation='relu',input_shape=(784,)))\n",
    "\n",
    "# Add the second hidden layer\n",
    "model.add(Dense(50,activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(10,activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X,y,validation_split=0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
